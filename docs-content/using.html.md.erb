---
title: Using Splunk Firehose Nozzle for PCF
owner: Partners
---

This topic describes how to use Splunk Firehose Nozzle for PCF. After installing & configuring Splunk Firehose Nozzle for PCF, PCF operators can then navigate in the browser to the URL of their existing Splunk Enterprise deployment to immediately search, report, visualize and alert on PCF Firehose data.

The following assumes basic familiarty on how to run searches, save reports, and create dashboards in Splunk Enterprise. If you're new to Splunk, start with the [Search Tutorial](https://docs.splunk.com/Documentation/Splunk/6.5.0/SearchTutorial/WelcometotheSearchTutorial) which will show you how to search data and create simple dashboards.

For a starter sample dashboard for operational intelligence, you can use community-supported [Splunk Add-on for Cloud Foundry](#add-on) which includes many pre-built panels and reports to help you quickly build your own custom dashboards to monitor the health of your foundation and the performance of your applications.

##<a id='search'></a>Search Firehose Data

The firehose event types forwared by the Splunk Nozzle are assigned the following Splunk sourcetypes by default:

  PCF Firehose event type | Splunk sourcetype
  --- | ---
  Error | `cf:error`
  HttpStartStop | `cf:httpstartstop`
  LogMessage | `cf:logmessage`
  ContainerMetric | `cf:containermetric`
  CounterEvent | `cf:counterevent`
  ValueMetric | `cf:valuemetric`

In addition, logs from the nozzle itself are of sourcetype `cf:splunknozzle`.

You can use these Splunk sourcetypes to search & retrieve the relevant events from the index, use statistical commands to calculate metrics and generate reports, search for specific conditions within a rolling time window, identify patterns in your data, predict future trends, and so on. For example:

  * Search for any errors from your foundation:

    `sourcetype=cf:error`

  * Search for router bad gateways:

    `sourcetype=cf:counterevent name="bad_gateways"`

  * Search for metrics of remaining cell disk capacity or memory:

    `sourcetype=cf:valuemetric name=CapacityRemainingDisk`
    `sourcetype=cf:valuemetric name=CapacityRemainingMemory`

This assumes you have access to the index specified to receive these firehose events. You may need to explicitly specify the index in your search if that index is not part of the indexes searched by default for the Splunk role you belong to. These searches can then be saved as reports and used to power dashboard panels, as illustrated in the next section.

##<a id='report'></a>Report & Visualize Firehose Data

To monitor your environment operational health, you can build on these same searches to generate reports & visualizations. For example:

  * Report available memory per cell:

    ```
    sourcetype=cf:valuemetric name=CapacityRemainingMemory
      | eval valueGB=round(case(unit=="MiB", value/1024, unit=="KiB", value/(1024*1024), unit=="GiB", value),2)
      | stats min(valueGB) as mem by job_instance
      | rename mem as "Available Memory (GB)", job_instance as "Job Instance"
    ```
    ![Report Example: Available mem per cell](images/report-available-memory-per-cell.png)

  * Report available memory per cell over time:

    ```
    sourcetype=cf:valuemetric name=CapacityRemainingMemory 
      | eval valueGB=round(case(unit=="MiB", value/1024, unit=="KiB", value/(1024*1024), unit=="GiB", value),2) 
      | timechart min(valueGB) by job_instance
    ```
    ![Report Example: Available mem per cell over time](images/report-available-memory-per-cell-over-time.png)

  * Report number of routes registered with trend indicator:

    ```
    sourcetype=cf:valuemetric name=RoutesTotal
      | timechart avg(value) as numRoutes
    ```
    ![Report Example: Total routes with trend](images/report-routes-total-with-trend.png)

To learn more about searching & reporting with Splunk, refer to [Splunk Docs](https://docs.splunk.com/Documentation/Splunk)
for complete list of manuals & references, from generating visualizations to creating alerts.

<%#

##<a id='add-on'></a>Use Splunk Add-on for Cloud Foundry

todo: Add link to Splunkbase here once it's live

![Report Example: Sample Ops Dashboard](images/dashboard-sample-cloud-foundry-add-on.png)

%>

##<a id='troubleshooting'></a> Advanced Troubleshooting

#### <a id='architecture'></a> Architecture Overview

Before troubleshooting it's useful to understand exactly what the tile installs.
Each vm contains two jobs:

* **splunk-nozzle**: This job is a small application that listens to the Cloud Foundry
[Loggregator's firehose](https://github.com/cloudfoundry/loggregator). When configured
to, the nozzle will also connect back to the Cloud Foundry API to collect and cache application metadata
(enriching events with additional information).
* **splunk-forwarder**: This job is a Splunk
[heavy forwarder](https://docs.splunk.com/Splexicon:Heavyforwarder). It sets up a local
[HTTP event collector](http://dev.splunk.com/view/event-collector/SP-CAAAE6M) to
receive events from the nozzle and then sends them on to the user-specified pre-configured indexer(s).

The logging system will round-robin events across the `splunk-nozzle` vm instances. Each
`splunk-nozzle` job will send the events it received locally to the co-located `splunk-forwarder`

**Splunk Firehose Nozzle** (or shorthand **Splunk Nozzle**) refers to
both `splunk-nozzle` and `splunk-forwarder` jobs as one entity in this context.

#### <a id='walkthrough'></a> Troubleshooting Walkthrough

##### 1) Ensure Splunk indexer(s) are listening to data on configured port

  Search internal logs of indexer(s), specifically the TCP input processor, as follows:

  ```
  index=_internal sourcetype=splunkd component=TcpInputProc 
  ```

  A correct setup will show an event as follows:

  *10-13-2016 17:31:16.226 +0000 INFO  TcpInputProc - Creating fwd data Acceptor for IPv4 port 9997 with SSL*

##### 2) Ensure Splunk Nozzle is connected to Splunk indexer(s):

  Search internal logs of nozzle, specifically the TCP output processor, to confirm Splunk nozzle is correctly forwarding:

  ```
  index=_internal host="splunk-nozzle_*" sourcetype=splunkd component=TcpOutputProc
  ```
  A correct setup will show an event as follows:

  *11-04-2016 20:24:13.702 +0000 INFO  TcpOutputProc - Connected to idx=<redacted>:9997 using ACK.*

  If no such event is found, then no data is being forwarded from nozzle. This can be due to mis-configured indexer(s) firewall, or mis-configured SSL settings of either nozzle (forwarder) or indexer(s). Search internal logs of Splunk indexer(s) for any errors:

  ```
  index=_internal sourcetype=splunkd component=TcpInputProc ERROR
  ```

  An example error would be:
  *11-03-2016 22:23:00.046 +0000 ERROR TcpInputProc - Error encountered for connection from src=<redacted>:55713. error:140760FC:SSL routines:SSL23_GET_CLIENT_HELLO:unknown protocol*

  Whether you are using self-signed certificates or certificates signed by a third party, ensure to follow these [instructions](http://docs.splunk.com/Documentation/Splunk/6.5.0/Security/HowtoprepareyoursignedcertificatesforSplunk) to provide your signed certificates details in correct format when configuring the tile. For more details on troubleshooting and validating your forwarder/receiver connection, refer to the following Splunk Docs resources:

  * http://docs.splunk.com/Documentation/Splunk/6.5.0/Forwarding/Receiverconnection
  * http://docs.splunk.com/Documentation/Splunk/6.5.0/Security/Validateyourconfiguration
  * http://docs.splunk.com/Documentation/Splunk/6.5.0/Security/Troubleshootyouforwardertoindexerauthentication

##### 3) Ensure Splunk Nozzle is forwarding events from the firehose:

  Search application logs of nozzle to confirm correct behavior:

  ```
  sourcetype="cf:splunknozzle"
  ```

  A correct setup will log number of firehose events logged as JSON objects, for example:

  ```json
  {
    "ip": "localhost",
    "job": "splunk-nozzle",
    "job_index": "-1",
    "log_level": 1,
    "logger_source": "splunk-nozzle-logger",
    "message": "splunk-nozzle-logger.Posting 635 events",
    "origin": "splunk_nozzle"
  }
  ```

  Search application logs of nozzle for any errors:

  ```
  sourcetype="cf:splunknozzle" data.error=*
  ```
  Errors will be logged with corresponding message and stacktrace.

##### 4) Additional troubleshooting from Nozzle VMs

In most cases, you would only need to troubleshoot from Splunk which is recommended as shown above. However, if you don't have access to Splunk or the Splunk Nozzle is still not forwarding any data (which includes internal logs), the second alternative is to access Splunk Nozzle directly. The pre-requisite for this section is [connecting to the PCF deployment's bosh](http://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html)

Having done that, the [bosh logs](https://bosh.io/docs/job-logs.html)
command is a useful place to start debugging.

```
bosh logs splunk-nozzle 0
```

This will download a local copy of the nozzle's logs and the control script that starts
the Splunk forwarder deamon. This could illuminate issues. For example, seeing

```
goroutine 28 [running]:
...
panic: Non-ok response code [503] from splunk: {"text":"Server is busy","code":9}
...
```

would point an operator toward failure in the communication between the nozzle and
its co-located Splunk forwarder.

To view the Splunk forwarder's internal logs, look through
`/var/vcap/packages/splunk/var/log` on the nozzle's vms.
 